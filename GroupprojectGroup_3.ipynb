{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfKIEVab9Rg+kO4fqHqDSj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2203a51759/AIML/blob/main/GroupprojectGroup_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train.py"
      ],
      "metadata": {
        "id": "2enWsj9RJNxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import argparse\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from model import build_model\n",
        "from datasets import get_datasets, get_data_loaders\n",
        "from utils import save_model, save_plots\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    '-e', '--epochs', type=int, default=20,\n",
        "    help='Number of epochs to train our network for'\n",
        ")\n",
        "parser.add_argument(\n",
        "    '-lr', '--learning-rate', type=float,\n",
        "    dest='learning_rate', default=0.001,\n",
        "    help='Learning rate for training the model'\n",
        ")\n",
        "args = vars(parser.parse_args())\n",
        "\n",
        "# Get the datasets and data loaders\n",
        "train_dataset, valid_dataset = get_datasets()\n",
        "train_loader, valid_loader = get_data_loaders(train_dataset, valid_dataset)\n",
        "\n",
        "# Build the model\n",
        "model = build_model(pretrained=True, fine_tune=False, num_classes=10)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=args['learning_rate'])\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(args['epochs']):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader):\n",
        "        images, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')\n",
        "\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_loader:\n",
        "            images, labels = batch\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "    accuracy = total_correct / len(valid_dataset)\n",
        "    print(f'Epoch {epoch+1}, Validation Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Save the model\n",
        "save_model(model, f'../outputs/model.pth')\n",
        "\n",
        "# Save the plots\n",
        "save_plots(train_losses, valid_losses, train_accuracies, valid_accuracies)"
      ],
      "metadata": {
        "id": "nlLfyLgWI2Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model.py\n"
      ],
      "metadata": {
        "id": "SQejsO07I7_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "def build_model(pretrained=True, fine_tune=False, num_classes=10):\n",
        "    if pretrained:\n",
        "        print('[INFO]: Loading pre-trained weights')\n",
        "    else:\n",
        "        print('[INFO]: Not loading pre-trained weights')\n",
        "    model = models.mobilenet_v3_large(pretrained=pretrained)\n",
        "\n",
        "    if fine_tune:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    # Replace the last layer with a new one\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Linear(model.classifier[0].in_features, num_classes)\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "D9l3hBRHJFsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset.py\n"
      ],
      "metadata": {
        "id": "G-jZOwpfJX2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "ROOT_DIR = '../input/rice_leaf_diseases'\n",
        "VALID_SPLIT = 0.1\n",
        "IMAGE_SIZE = 224\n",
        "\n",
        "def get_datasets():\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.CenterCrop(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    dataset = datasets.ImageFolder(ROOT_DIR, transform=data_transforms)\n",
        "    indices = list(range(len(dataset)))\n",
        "    split = int(VALID_SPLIT * len(dataset))\n",
        "    train_indices, valid_indices = indices[split:], indices[:split]\n",
        "\n",
        "    train_dataset = Subset(dataset, train_indices)\n",
        "    valid_dataset = Subset(dataset, valid_indices)\n",
        "\n",
        "    return train_dataset, valid_dataset\n",
        "\n",
        "def get_data_loaders(train_dataset, valid_dataset):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    return train_loader, valid_loader\n"
      ],
      "metadata": {
        "id": "5BHZF-hOJK2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utlis.py"
      ],
      "metadata": {
        "id": "6mGrc7LrJsBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def save_model(model, path):\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "def save_plots(train_losses, valid_losses, train_accuracies, valid_accuracies):\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(valid_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('../outputs/loss.png')\n",
        "\n",
        "    plt.plot(train_accuracies, label='Training Accuracy')\n",
        "    plt.plot(valid_accuracies, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.savefig('../outputs/accuracy.png')\n"
      ],
      "metadata": {
        "id": "__H7-pAWJvjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "interfence.py"
      ],
      "metadata": {
        "id": "H64ayDf2J32F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "def"
      ],
      "metadata": {
        "id": "MyKZgDAQJ8a_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}